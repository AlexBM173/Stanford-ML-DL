{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e394fba6",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "\n",
    "A key part of most machine learning problems is finding model parameters that minimise the cost function of an algorithm given a dataset. One of the simplest methods that serves as the basis of other techniques is gradient descent, where model parameters are continually updated by taking steps against the gradient of the loss function. As a result, parameter values will eventually tend to a local minimum.\n",
    "\n",
    "## The algorithm\n",
    "\n",
    "Say we have a cost function $C(\\mathbf{\\theta})$ which we want to minimise with respect to the parameters $\\mathbf{\\theta}$. For each step of gradient descent, we will update the parameters by subtracting a multiple of the gradient of the cost function:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\theta}_{t+1}:=\\mathbf{\\theta}_t-\\alpha\\nabla_{\\mathbf{\\theta}}C(\\mathbf{\\theta})\n",
    "\\end{equation}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
